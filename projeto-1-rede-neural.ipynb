{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede neural do zero\n",
    "\n",
    "## Objetivo do projeto\n",
    "\n",
    "### Rede neural\n",
    "- O que é\n",
    "\n",
    "### mNist\n",
    "- O que é\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor() # definindo a conversão de imagem para tensor\n",
    "\n",
    "trainset = datasets.MNIST('./MNIST_data', download=True, train=True, transform=transform) # carrega a parte de treino do dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # cria um buffer para pegar os dados por partes\n",
    "\n",
    "valset = datasets.MNIST('./MNIST_data', download=True, train=False, transform=transform) # carrega a parte de treino do dataset\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) # cria um buffer para pegar os dados por partes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando os dados do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12a03804500>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "imagens, etiquetas = dataiter._next_data()\n",
    "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando o tamanho do tensor que representa uma imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(imagens[0].shape) # para verificar as dimensões do tensor de cada imagem\n",
    "print(etiquetas[0].shape) # para verificar as dimensões do tensor de cada etiqueta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando a rede neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras - InceptionV3](https://keras.io/api/applications/inceptionv3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Modelo, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 128) # camada de entrada, 784 neurônios que ligam a 128\n",
    "        self.linear2 = nn.Linear(128, 64) # camada interna 1, 128 neurônios que se ligam a 64\n",
    "        self.linear3 = nn.Linear(64, 10) # camada interna 2, 64 neurônios que se ligam a 10\n",
    "        # para a cama de saída não é necessário definir nada, pois só precisamos pegar o output da camada interna 2\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.linear1(X)) # função de ativação da camada de entrada para a camada interna 1\n",
    "        X = F.relu(self.linear2(X)) # função de ativação da camada interna 1 para a camada interna 2\n",
    "        X = self.linear3(X) # função de ativação da camada interna 2 para a camada de saída, nesse caso f(x) = x\n",
    "        return F.log_softmax(X, dim=1) # dados utilizados para calcular a perda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando o otimizador da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treino(modelo, trainloader, device):\n",
    "    otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) # define a política de atualização dos pesos e do bias\n",
    "    inicio = time() # timer para sabermos quanto tempo levou o treino\n",
    "\n",
    "    criterio = nn.NLLLoss() # definindo o critério para calcular a perda\n",
    "    EPOCHS = 10 # número de epochs que o algorítmo rodará\n",
    "    modelo.train() # ativando o modo de treinamento do modelo\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        perda_acumulada = 0 # inicialização da perda acumulada da epoch em questão\n",
    "\n",
    "        for imagens , etiquetas in trainloader:\n",
    "\n",
    "            imagens = imagens.view(imagens.shape[0], -1) # convertendo as imagens para \"vetores\" de 28*28 casas para ficarem compatíveis com \n",
    "            otimizador.zero_grad() # zerando os gradientes por conta do ciclo anterior\n",
    "\n",
    "            output = modelo(imagens.to(device)) # colocando os dados no modelo\n",
    "            perda_instantanea = criterio(output, etiquetas.to(device)) # calculando a perda da epoch em questão\n",
    "\n",
    "            perda_instantanea.backward() # back propagation a partir da perda\n",
    "\n",
    "            otimizador.step() # atualizando os pesoas e o bias\n",
    "\n",
    "            perda_acumulada += perda_instantanea.item() # atualização da perda acumulada\n",
    "\n",
    "        else:\n",
    "            print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
    "    print(\"\\nTempo de treino (em minutos) = \", (time()-inicio)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacao(modelo, valloader, device):\n",
    "    conta_corretas, conta_todas = 0, 0\n",
    "    for imagens, etiquetas in valloader:\n",
    "        for i in range(len(etiquetas)):\n",
    "            img = imagens[i].view(1, 784)\n",
    "            # desativar o autograd para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento\n",
    "            with torch.no_grad():\n",
    "                logps = modelo(img.to(device)) # output do modelo em escala logarítmica\n",
    "\n",
    "            ps = torch.exp(logps) # converte output para escala normal (lembrando que é um tensor)\n",
    "            probab = list(ps.cpu().numpy()[0])\n",
    "            etiqueta_pred = probab.index(max(probab)) # converte o tensor em um número, no caso, o número que o modelo previu como resultado\n",
    "            etiqueta_certa = etiquetas.numpy()[i]\n",
    "            if (etiqueta_certa == etiqueta_pred): # compara a previsão com o valor correto\n",
    "                conta_corretas += 1\n",
    "            conta_todas += 1\n",
    "\n",
    "    print(\"Total de imagens testadas =\", conta_todas)\n",
    "    print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Modelo(\n",
       "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = Modelo() # inicializa o modelo\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # modelo rodará na GPU se possível\n",
    "modelo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Perda resultante: 1.1499647231562051\n",
      "Epoch 2 - Perda resultante: 0.3865626864691279\n",
      "Epoch 3 - Perda resultante: 0.3165928352949843\n",
      "Epoch 4 - Perda resultante: 0.27946614779865564\n",
      "Epoch 5 - Perda resultante: 0.25020433340801485\n",
      "Epoch 6 - Perda resultante: 0.22489393127561885\n",
      "Epoch 7 - Perda resultante: 0.20289912928284995\n",
      "Epoch 8 - Perda resultante: 0.18363648492580792\n",
      "Epoch 9 - Perda resultante: 0.16661958486191245\n",
      "Epoch 10 - Perda resultante: 0.15161444061695895\n",
      "\n",
      "Tempo de treino (em minutos) =  1.7891513586044312\n"
     ]
    }
   ],
   "source": [
    "treino(modelo=modelo, trainloader=trainloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de imagens testadas = 10000\n",
      "\n",
      "Precisão do modelo = 95.45%\n"
     ]
    }
   ],
   "source": [
    "validacao(modelo=modelo, valloader=valloader, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
